# Registry-Driven Push Architecture

This document describes the data flow from raw source landing through to the analytics star schema. The architecture is contract-first: routing is determined by physical schema fingerprints, and every record carries its original payload for downstream replay.

---

## 1. Core Principles

### Physical Truth over Convention
Routing decisions are not made by naming conventions or manual configuration. When data lands in the warehouse, the platform calculates an MD5 hash of the physical column names and data types — the **source_schema_hash** — creating a unique structural fingerprint. This fingerprint is matched against the `connector_blueprints` registry to determine which master model sink should receive the data.

### Contract Stability via Pydantic
Python-native Pydantic schemas serve as the source of truth for landing table structures. All fields are `Optional` with `None` defaults to prevent NOT NULL parser errors during DuckDB schema evolution. The schemas are "frozen" — once a connector version is published, its schema is immutable. New fields are additive only.

### Raw Payload Preservation
Every staging model re-bundles the flattened relational columns from a source table into a single `raw_data_payload` JSON column before pushing to the master model. This means the master model contains both the structural metadata (tenant_slug, source_platform, source_schema_hash, loaded_at) and the complete original record. Downstream models can extract any field from this JSON without re-ingesting from the source.

### Tenant Isolation by Layer
Master models are intentionally multi-tenant — they pool all tenants' records for a given source object into a single table. Tenant isolation is enforced at the intermediate layer, where each model filters by `WHERE tenant_slug = '{slug}' AND source_platform = '{platform}'`. This keeps the master model count manageable (31 models for 13 connectors) while giving each tenant a clean, isolated view of their data.

---

## 2. Layer-by-Layer Walkthrough

### Layer 1: Sources — `models/sources/{tenant}/{platform}/_sources.yml`

Source shims that declare raw landing tables as dbt sources. Each tenant has one directory per enabled platform. These are generated by `scripts/onboard_tenant.py` based on `tenants.yaml`.

Example: `models/sources/tyrell_corp/facebook_ads/_sources.yml` declares tables like `tyrell_corp_facebook_ads__ads`, `tyrell_corp_facebook_ads__campaigns`, etc.

**Count:** ~30 source declarations across 2 active tenants.

### Layer 2: Staging — `models/staging/{tenant}/{platform}/`

Staging models are "pushers" — they read from the raw source tables, enrich with metadata, bundle the original columns into `raw_data_payload` JSON, and trigger the `sync_to_master_hub()` post-hook to MERGE into the correct master model.

Each staging model:
1. Reads from its `_sources.yml` declaration
2. Generates a `tenant_skey` surrogate key via `gen_tenant_key()`
3. Packs all source columns into `raw_data_payload` using `to_json()` or struct construction
4. Runs `sync_to_master_hub()` as a post-hook, which performs an INSERT/MERGE into the target master model

Naming convention: `stg_{tenant_slug}__{platform}_{object}.sql`

Example: `stg_tyrell_corp__facebook_ads_campaigns.sql` reads from the `tyrell_corp_facebook_ads__campaigns` source, packs the data, and pushes to `platform_mm__facebook_ads_api_v1_campaigns`.

**Count:** 28 staging models across 2 active tenants.

### Layer 3: Platform — Master Models, Hubs, Satellites, Ops

#### Master Models — `models/platform/master_models/`
The definitive multi-tenant sinks. Each master model corresponds to one object from one connector's API version. They are append-only tables with the following standard columns:

```sql
tenant_slug         VARCHAR    -- Which tenant owns this record
source_platform     VARCHAR    -- Which connector produced it (e.g., 'facebook_ads', 'instagram_ads')
tenant_skey         VARCHAR    -- Surrogate key (MD5 of tenant + platform + object primary key)
source_schema_hash  VARCHAR    -- MD5 fingerprint of the source table's physical schema
loaded_at           TIMESTAMP  -- When the record was pushed
raw_data_payload    JSON       -- The complete original record as JSON
```

Facebook Ads and Instagram Ads share master models (both use `facebook_ads_api_v1`) because they share the Meta API. They are differentiated by `source_platform`.

**Count:** 31 master models covering all 13 connectors.

#### Hub Key Registry — `models/platform/hubs/hub_key_registry.sql`
Tracks all tenant surrogate keys across the platform for cross-referencing and deduplication.

#### Satellites — `models/platform/satellites/`
Data Vault-style satellite tables that capture state changes over time:
- `platform_sat__source_schema_history` — Versions physical schema fingerprints per source table
- `platform_sat__tenant_config_history` — Versions tenant configuration changes from `tenants.yaml`
- `platform_sat__tenant_source_configs` — Current snapshot of each tenant's enabled sources

#### Ops — `models/platform/ops/`
Operational metadata models:
- `platform_ops__master_model_registry` — Catalog of all master models and their connector mappings
- `platform_ops__schema_hash_registry` — Maps schema hashes to their physical column definitions
- `platform_ops__boring_semantic_layer` — Aggregates semantic manifests per tenant (being upgraded to BSL)

#### Observability — `models/platform/observability/`
Captures dbt execution metadata via `on-run-end` hooks (defined in `macros/dbt_metadata/metadata_ops.sql`):
- `model_artifacts__current` — Every model's node_id, dependencies, materialization, tags, config
- `run_results__current` — Execution status, timing, rows affected per model per run
- `test_artifacts__current` — Test definitions and their target models

These tables are populated by the `upload_model_definitions()`, `upload_run_results()`, and `upload_test_definitions()` macros that run automatically after each `dbt run`.

### Layer 4: Intermediate — `models/intermediate/{tenant}/{platform}/`

Tenant-isolated extraction models. Each model:
1. Reads from a specific master model via `{{ ref('platform_mm__...') }}`
2. Filters by `WHERE tenant_slug = '{slug}' AND source_platform = '{platform}'`
3. Extracts typed fields from `raw_data_payload` using DuckDB JSON syntax (`->>` for scalar, `->` for nested objects)
4. Applies tenant-specific logic from `tenants.yaml` (conversion events, funnel steps, cost_micros conversion, etc.)
5. Preserves the original `raw_data_payload` for downstream use

Naming convention: `int_{tenant_slug}__{platform}_{object}.sql`

All intermediate models are materialized as `view` to avoid data duplication — they are lightweight projections over the master model data.

Example: `int_tyrell_corp__google_analytics_events.sql` extracts `event_name`, `user_pseudo_id`, `traffic_source`, `purchase_revenue`, etc. from the GA4 events master model, and applies the `conversion_events: ['purchase']` logic from tyrell_corp's tenant config.

**Count:** 28 intermediate models across 2 active tenants (16 for tyrell_corp, 12 for wayne_enterprises).

### Layer 5: Analytics — `models/analytics/{tenant}/`

The star schema. Thin shell models that call factory macros to assemble facts and dimensions from the intermediate layer.

Each tenant gets:
- `fct_{tenant}__ad_performance.sql` — Calls `build_fct_ad_performance(tenant, ad_sources_list)`
- `fct_{tenant}__orders.sql` — Calls `build_fct_orders(tenant, ecommerce_sources_list)`
- `fct_{tenant}__sessions.sql` — Calls `build_fct_sessions(tenant, analytics_source, conversion_events)`
- `dim_{tenant}__campaigns.sql` — Calls `build_dim_campaigns(tenant, ad_sources_list)`

All analytics models are materialized as `table` for query performance.

The factory macros use an `engine_map` dictionary to look up which engine macro to call for each source. If a source isn't in the tenant's list, it's skipped. If no valid sources are found, the factory emits a `WHERE 1 = 0` empty result set with the correct column types.

**Count:** 8 analytics models across 2 active tenants (4 per tenant).

---

## 3. The Macro Library

### Onboarding Macros (`macros/onboarding/`)
| Macro | Purpose |
|:------|:--------|
| `generate_staging_pusher` | Generates the staging model SQL that bundles source columns into `raw_data_payload` |
| `sync_to_master_hub` | Post-hook macro that MERGEs staging records into the target master model |
| `sync_to_schema_history` | Records schema fingerprint changes in the satellite table |
| `load_clients` | Loads tenant configurations from `tenants.yaml` into the dbt context |

### Engine Macros (`macros/engines/`)
| Domain | Engines | Canonical Output |
|:-------|:--------|:-----------------|
| Paid Ads | facebook, instagram, google, bing, linkedin, amazon, tiktok | report_date, campaign_id, ad_group_id, ad_id, spend, impressions, clicks, conversions |
| Ecommerce | shopify, bigcommerce, woocommerce | order_id, order_date, total_price, currency, financial_status, customer_email, customer_id, line_items_json |
| Analytics | google_analytics, amplitude, mixpanel | session_id, user_pseudo_id, session_start/end_ts, traffic_source/medium/campaign, is_conversion_session, session_revenue |

### Factory Macros (`macros/factories/`)
| Factory | Input | Output |
|:--------|:------|:-------|
| `build_fct_ad_performance(tenant, sources)` | List of ad platform names | UNION ALL of matching engines |
| `build_fct_orders(tenant, sources)` | List of ecommerce platform names | UNION ALL of matching engines |
| `build_fct_sessions(tenant, source, events)` | Single analytics platform + conversion events | Single engine call |
| `build_dim_campaigns(tenant, sources)` | List of ad platform names | UNION ALL of campaign intermediate models |

### Utility Macros (`macros/utils/`)
| Macro | Purpose |
|:------|:--------|
| `extract_json(field, type, source_column)` | Type-safe JSON field extraction from `raw_data_payload` |
| `gen_tenant_key(columns)` | Generates MD5 surrogate key for tenant records |
| `get_tenant_config(slug)` | Reads tenant config from `tenants.yaml` |
| `get_client_logic(slug, platform, table)` | Retrieves table-level logic overrides |
| `apply_tenant_logic(slug, platform, table)` | Injects WHERE clauses from tenant config |

### Metadata Macros (`macros/dbt_metadata/metadata_ops.sql`)
On-run-end hooks that capture dbt execution artifacts:
- `init_artifact_tables()` — Creates artifact tables on first run / full refresh
- `upload_model_definitions()` — Captures model DAG, config, tags, dependencies
- `upload_run_results()` — Captures execution status, timing, rows affected
- `upload_test_definitions()` — Captures test definitions and targets

---

## 4. Configuration Files

### `tenants.yaml` — The Single Source of Truth
Defines every tenant's identity, status, enabled sources, mock data generation parameters, and table-level business logic. This file drives:
- Mock data generation (what data to synthesize and how much)
- dbt model scaffolding (which source/staging/intermediate models to create)
- Analytics model configuration (which sources to pass to factories)
- Semantic layer population (which conversion events, funnel steps, etc.)

### `supported_connectors.yaml` — The Connector Catalog
Defines every supported data source, its API version, master model ID, and available objects. This is the "menu" of sources that tenants can enable. Adding a new connector here (plus its engine, schema, and master model) makes it available to all tenants.

---

## 5. Guardrails

1. **DNA-Based Routing**: Staging-to-master-model routing is hard-coded in Python based on physical column signatures. No dynamic dbt lookups at runtime.
2. **Contract Stability**: All Pydantic fields are `Optional` with `None` defaults. DuckDB schema evolution is handled gracefully.
3. **Tenant Isolation**: Master models are multi-tenant. Isolation happens at the intermediate layer via `WHERE tenant_slug`. No cross-tenant data leakage in analytics.
4. **Source Agnosticism**: Engines and factories produce identical column contracts regardless of input source. NULL columns are explicitly typed when a source lacks granularity.
5. **Raw Payload Preservation**: `raw_data_payload` is never dropped. Every layer downstream of master models can re-extract fields without re-ingesting.
6. **Deterministic Semantics**: The semantic layer sits on the star schema (facts + dimensions), not on raw or intermediate data, to ensure governed and accurate queries.

---

## 6. Order of Operations (Building from Scratch)

1. **Initialize Connector Library** — Run `initialize_connector_library.py` to populate `connector_blueprints` with schema fingerprints for all 13 connectors.
2. **Configure Tenants** — Define tenants and their source mix in `tenants.yaml`.
3. **Generate Mock Data** — Run mock data generators to land synthetic data in DuckDB/MotherDuck.
4. **Onboard Tenant Models** — Run `onboard_tenant.py` to scaffold source, staging, and intermediate models.
5. **Create Analytics Shells** — Write 4 shell models per tenant in `models/analytics/{tenant}/` calling the appropriate factories.
6. **Run dbt** — Execute `dbt run` to materialize the full pipeline. On-run-end hooks capture metadata.
7. **Populate Semantic Layer** — Post-run script reads metadata + introspects analytics tables → generates BSL definitions.
8. **Serve API** — FastAPI exposes per-tenant semantic endpoints + MCP for AI agents.
