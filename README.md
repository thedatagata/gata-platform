# GATA Platform

**Multi-Tenant Analytics Platform with Automated Onboarding, Source-Agnostic Star Schemas, and a Two-Tier Semantic Layer**

**Status:** Alpha — Backend transformation pipeline fully operational (136 models, 133 PASS), frontend integration in progress
**Stack:** dlt / dbt / DuckDB / MotherDuck / Deno Fresh / FastAPI / Boring Semantic Layer

---

## What This Project Is

GATA Platform is a multi-tenant data platform that demonstrates how a single analytics engineer can architect and build a production-grade system covering ingestion, transformation, governance, and analytics serving. The project solves a real problem in the marketing analytics space: businesses use different combinations of ad platforms, ecommerce tools, and analytics products, but they all need the same reporting outputs.

The platform:

1. **Onboards tenants automatically** — a new customer selects their data sources, and the platform scaffolds ingestion, staging, and reporting models without manual SQL. The onboarding script generates dbt models, registers schema hashes, and wires up the push circuit.
2. **Produces identical star schemas regardless of source mix** — whether a tenant uses Shopify + Facebook Ads or BigCommerce + Bing Ads, the analytics layer outputs the same fact and dimension tables through an engine/factory macro architecture. 13 source-specific engines normalize to 6 canonical star schema tables per tenant.
3. **Preserves raw data for logic replay** — every record carries its original JSON payload alongside extracted fields, so when business logic changes (conversion event definitions, attribution models, UTM mappings), the reporting layer can be rebuilt from raw data without re-ingesting.
4. **Tracks configuration state over time** — Data Vault-inspired hubs and satellites version tenant configs and schema fingerprints, so when a tenant changes their conversion event definition, the platform knows exactly what changed, when, and can replay the new logic against historical raw data.
5. **Exposes analytics through a two-tier semantic layer** — lightweight in-browser text-to-SQL for single-table exploration (DuckDB WASM + Qwen 2.5 Coder), and a backend Boring Semantic Layer for governed multi-table OLAP queries served via FastAPI.

The project uses fictional tenants (Tyrell Corporation, Wayne Enterprises, Stark Industries) with synthetic data generated by mock data engines. No proprietary data or client IP is involved.

---

## Architecture Overview

### The Data Flow

```
+-----------------------------------------------------------------------+
|                         INGESTION (dlt)                                |
|  Mock Data Generators -> Pydantic Schemas -> DuckDB/MotherDuck Landing |
+-----------------------------------+-----------------------------------+
                                    |
+-----------------------------------v-----------------------------------+
|                      TRANSFORMATION (dbt)                              |
|                                                                        |
|  sources/{tenant}/{platform}/       <- Source shims (_sources.yml)      |
|       |                                                                |
|  staging/{tenant}/{platform}/       <- JSON bundling + metadata        |
|       |                                post-hook: sync_to_master_hub() |
|       |                                                                |
|  platform/master_models/            <- Multi-tenant sinks (31 models)  |
|       |                                One per connector object         |
|       |                                Contains raw_data_payload JSON   |
|       |                                                                |
|  platform/hubs + satellites/        <- Config & schema versioning      |
|       |                                Data Vault state tracking        |
|       |                                                                |
|  intermediate/{tenant}/{platform}/  <- Tenant isolation + extraction   |
|       |                                Filters by tenant_slug          |
|       |                                Typed fields from JSON           |
|       |                                                                |
|  analytics/{tenant}/                <- Star schema (facts + dims)      |
|                                        Engine macros normalize sources |
|                                        Factory macros union by mix     |
+-----------------------------------+-----------------------------------+
                                    |
+-----------------------------------v-----------------------------------+
|                      SEMANTIC LAYER (Two-Tier)                         |
|                                                                        |
|  Tier 1: In-Browser                                                    |
|    DuckDB WASM loads single table -> auto-profiler generates metadata  |
|    -> WebLLM (Qwen 2.5 Coder 3B) generates SQL from natural language   |
|    -> SemanticReportObj executes against WASM instance                  |
|    -> AutoChart renders visualization                                  |
|    Purpose: Single-table EDA, no backend dependency                    |
|                                                                        |
|  Tier 2: Backend BSL (in progress)                                     |
|    Per-tenant YAML configs define dimensions, measures, join paths     |
|    -> FastAPI serves semantic endpoints per tenant                      |
|    -> Query builder compiles structured queries to SQL                  |
|    -> MotherDuck executes governed multi-table joins                    |
|    Purpose: Multi-table OLAP, governed join paths, deterministic SQL   |
+------------------------------------------------------------------------+
```

---

## Registry-Driven Push Architecture

Data flows from raw landing tables to the star schema through a five-layer pipeline. Routing decisions are based on physical schema fingerprints, not naming conventions.

### Layer 1: Sources

Auto-generated `_sources.yml` files that declare raw landing tables as dbt sources. Each tenant gets one directory per enabled platform. Created by `scripts/onboard_tenant.py`.

### Layer 2: Staging (The Push Circuit)

Staging models are "pushers." Each one is a view generated by the `generate_staging_pusher` macro that:

1. SELECTs all columns from the raw source table
2. Adds metadata: `tenant_slug`, `tenant_skey` (MD5 surrogate key), `source_platform`, `source_schema_hash`
3. Packs the entire source row into a single `raw_data_payload` JSON column via `row_to_json(base)`
4. Fires `sync_to_master_hub()` as a **post-hook** — after the view is created, the macro runs a MERGE INTO the target master model

The MERGE deduplicates on `tenant_slug + source_platform + MD5(raw_data_payload)`, so re-running the pipeline is idempotent. The target master model is determined by the `source_schema_hash` — a structural fingerprint of the source table's columns and types, matched against the `connector_blueprints` registry.

```sql
-- What sync_to_master_hub does:
MERGE INTO platform_mm__shopify_api_v1_orders AS target
USING stg_tyrell_corp__shopify_orders AS source
ON target.tenant_slug = source.tenant_slug
   AND target.source_platform = source.source_platform
   AND md5(target.raw_data_payload::VARCHAR) = md5(source.raw_data_payload::VARCHAR)
WHEN NOT MATCHED THEN INSERT (...)
```

### Layer 3: Master Models (Multi-Tenant Sinks)

31 master model tables, one per connector object (e.g., `platform_mm__shopify_api_v1_orders`, `platform_mm__facebook_ads_api_v1_facebook_insights`). Every record contains:

| Column | Purpose |
|:-------|:--------|
| `tenant_slug` | Which tenant owns this record |
| `source_platform` | Which connector produced it (e.g., `facebook_ads` vs `instagram_ads`) |
| `tenant_skey` | MD5 surrogate key for the tenant + platform + object primary key |
| `source_schema_hash` | Structural fingerprint of the source table |
| `raw_data_payload` | The complete original record as JSON |
| `loaded_at` | When the record was pushed |

Facebook Ads and Instagram Ads share master models (`facebook_ads_api_v1`) because they use the same Meta API. They are differentiated by `source_platform` downstream.

### Layer 4: Intermediate (Tenant Isolation + Field Extraction)

20 intermediate models (8 Tyrell, 6 Wayne, 6 Stark) that filter by `tenant_slug` and extract typed fields from `raw_data_payload` using DuckDB JSON syntax:

```sql
-- Scalar extraction with type cast
CAST(raw_data_payload->>'$.total_price' AS DOUBLE) AS total_price

-- Nested JSON kept as JSON
raw_data_payload->'$.line_items' AS line_items_json

-- Computed columns (Google Ads stores spend in micros)
CAST(raw_data_payload->>'$.cost_micros' AS BIGINT) / 1000000.0 AS spend
```

Intermediate models also apply tenant-specific business logic from `tenants.yaml` — conversion event definitions, funnel step mappings, and attribution rules. Most use the `generate_intermediate_unpacker` macro; models with computed columns or complex JSON (line items, event params) are written as raw SQL.

### Layer 5: Analytics (Star Schema)

18 shell models (6 per tenant) that call factory macros. Each factory looks up engines by source name and UNION ALLs the results. If a source isn't in the tenant's config, it's skipped.

---

## Shell and Engine Architecture

The reporting layer uses a **Shell and Engine** pattern to decouple source-specific logic from the star schema contract:

- **Engines** (`macros/engines/`) are dbt macros that know how to read a specific source's intermediate model and normalize it to a canonical schema. There are 13 engines across 3 domains: 7 paid ads, 3 ecommerce, 3 analytics.
- **Factories** (`macros/factories/`) are dbt macros that accept a tenant slug and a list of enabled sources, look up the correct engine for each source, and UNION ALL the results. There are 6 factories — one per star schema table.
- **Shell Models** (`models/analytics/{tenant}/`) are thin SQL files that call the appropriate factory with the tenant's source list from config. These are the only models created per tenant — engines and factories are shared.

This means `fct_tyrell_corp__ad_performance.sql` and `fct_wayne_enterprises__ad_performance.sql` both produce the same column contract (`tenant_slug, source_platform, report_date, campaign_id, ad_group_id, ad_id, spend, impressions, clicks, conversions`) despite pulling from completely different source platforms.

### Engine and Factory Contracts

**Paid Ads** — All 7 engines normalize to:
```
tenant_slug | source_platform | report_date | campaign_id | ad_group_id | ad_id
spend | impressions | clicks | conversions
```
Sources that lack granularity (e.g., Bing's account-level report has no `campaign_id`) emit `CAST(NULL AS VARCHAR)` for missing columns.

**Ecommerce** — All 3 engines normalize to:
```
tenant_slug | source_platform | order_id | order_date | total_price | currency
financial_status | customer_email | customer_id | line_items_json
```

**Analytics (Sessions)** — All 3 engines produce sessionized output:
```
tenant_slug | source_platform | session_id | user_pseudo_id | user_id
session_start_ts | session_end_ts | session_duration_seconds | events_in_session
traffic_source | traffic_medium | traffic_campaign | geo_country | device_category
is_conversion_session | session_revenue | transaction_id
```
Google Analytics and Mixpanel perform 30-minute inactivity sessionization. Amplitude uses native `session_id`. All accept a `conversion_events` list parameter for the `is_conversion_session` flag.

**Analytics (Events)** — All 3 engines also produce a raw event stream:
```
tenant_slug | source_platform | event_id | event_name | event_timestamp
user_pseudo_id | user_id | traffic_source | traffic_medium | traffic_campaign
geo_country | device_category | page_path | ecommerce_order_id | ecommerce_revenue
```

**Users (Dimension)** — Identity resolution across analytics + ecommerce:
```
tenant_slug | source_platform | user_key | user_pseudo_id | user_id
customer_email | customer_id | first_seen_at | last_seen_at | total_sessions
total_events | total_orders | total_revenue | is_customer
```

**Campaigns (Dimension)** — Campaign metadata across ad platforms:
```
tenant_slug | source_platform | campaign_id | campaign_name | campaign_status
```

---

## Automated Onboarding

Adding a new tenant to the platform is a two-command process. No SQL is written by hand.

### Step 1: Initialize the Connector Library

`scripts/initialize_connector_library.py` builds the `connector_blueprints` registry that maps physical schema fingerprints to master models. For each of the 13 supported connectors, it:

1. Creates a dummy tenant with only that connector enabled
2. Runs the mock data orchestrator to generate a sample dlt schema
3. Computes an MD5 hash of each table's column names and types (the "DNA fingerprint")
4. Registers the mapping: `schema_hash -> master_model_id` (e.g., `a3f7c2... -> shopify_api_v1_orders`)

This only needs to run once (or when connector schemas change). The registry is stored in `connector_blueprints` table in DuckDB.

### Step 2: Onboard a Tenant

`scripts/onboard_tenant.py <slug> --target sandbox` does everything else:

1. **Reads `tenants.yaml`** to find the tenant's enabled sources, generation parameters, and table-level logic
2. **Generates mock data** via `MockOrchestrator` — Pydantic-validated synthetic records landed into DuckDB/MotherDuck by dlt
3. **Calculates schema fingerprints** for each landed table and looks up the correct `master_model_id` from `connector_blueprints`
4. **Generates dbt source shims** — `_sources.yml` files in `models/sources/{tenant}/{platform}/` declaring the raw tables
5. **Generates staging pushers** — One `.sql` file per source table calling `generate_staging_pusher(tenant_slug, source_name, schema_hash, master_model_id, source_table)`. Each includes the `sync_to_master_hub()` post-hook

After onboarding, `dbt run` materializes the full pipeline: staging views fire their post-hooks to MERGE data into master models, intermediate models extract typed fields, and analytics shell models assemble the star schema.

### What the Fingerprint-Based Routing Solves

The schema hash approach means routing is determined by **physical truth** (what columns a table actually has), not naming conventions. If a connector's API version changes and produces different columns, the hash changes, and the router either matches a known blueprint or flags the table as unknown. This prevents silent data corruption from schema drift.

---

## Hubs and Satellites (State Tracking)

The platform uses Data Vault-inspired hubs and satellites to track configuration and schema changes over time. This is what makes logic replay possible — when a tenant changes their conversion event from `purchase` to `checkout_complete`, the platform knows what changed, when, and can rebuild the reporting layer from the raw JSON in master models.

### Tenant Config History (`platform_sat__tenant_config_history`)

The core satellite. It reads the tenant manifest from `dbt_project.yml` and unpacks the nested JSON structure into individual rows per tenant + source + table:

```
tenant_slug | source_name | table_name | table_logic | logic_hash | updated_at
```

The `table_logic` column contains the raw JSON business logic for that specific table (conversion events, funnel steps, etc.), and `logic_hash` is its MD5. When a tenant's `tenants.yaml` config changes, a new row appears with a different `logic_hash`. The model deduplicates with `QUALIFY ROW_NUMBER() OVER (PARTITION BY tenant_slug, source_name, table_name, logic_hash ORDER BY updated_at DESC) = 1`.

This is what enables logic replay: the historical raw data in master models doesn't change, but the logic applied to it (which events count as conversions, how UTMs are mapped) is versioned. Rebuilding the reporting layer applies the current logic version to all historical data.

### Hub Key Registry (`hub_key_registry`)

Generates surrogate keys for every tenant + source + table combination by reading from the config history satellite. Tracks:

```
client_slug | platform_name | table_name | tenant_skey | logic_config_hash | logic_version_at
```

This provides a single registry of all tenant keys across the platform for cross-referencing and deduplication.

### Source Schema History (`platform_sat__source_schema_history`)

A sink table populated by post-hooks during staging. Tracks physical schema fingerprints over time:

```
tenant_slug | platform_name | source_table_name | source_schema_hash | source_schema | first_seen_at
```

When a source API changes its column structure, the new fingerprint appears here. Combined with the `connector_blueprints` registry, this enables detection of schema drift — if a table's fingerprint doesn't match any known blueprint, the pipeline flags it.

### Tenant Source Configs (`platform_sat__tenant_source_configs`)

A latest-snapshot view over the config history — the current active configuration per tenant + source + table. Used by operational models for quick lookups without scanning the full history.

---

## Observability and Metadata Pipeline

The platform captures its own execution metadata via dbt hooks, creating a self-describing DAG that tracks what ran, what passed, what failed, and how the pipeline's models relate to each other.

### Artifact Capture (on-run-start / on-run-end)

The `metadata_ops.sql` macro file defines hooks that run automatically with every `dbt run`:

**on-run-start: `init_artifact_tables()`**
Creates (or recreates on full refresh) three artifact tables in a `dbt_artifacts` schema:
- `model_artifacts__current` — every model's node ID, name, materialization, dependencies, tags, config
- `run_results__current` — execution status, timing, rows affected per model
- `test_artifacts__current` — test definitions and their target models

**on-run-end: `upload_*()` macros**
After all models execute:
- `upload_model_definitions()` inserts every model in the DAG with its dependency graph (`depends_on_nodes` as JSON), materialization strategy, and metadata
- `upload_run_results()` inserts per-model execution results: status (success/error/skip), `rows_affected`, `execution_time_seconds`, `invocation_id`
- `upload_test_definitions()` inserts all test specs with their target model IDs

These use batch inserts (25 rows per batch) to avoid DuckDB parameter limits, and a `_safe_str()` helper macro for proper escaping.

### Observability Models

The artifact tables feed a layered observability pipeline:

**Staging** (ephemeral + incremental):
- `stg_platform_observability__model_definitions` — cleans and renames artifact fields, extracts `model_owner` from `meta` JSON
- `stg_platform_observability__run_results` — incremental on `[invocation_id, node_id]`, injects `dlt_load_id` from pipeline vars for lineage

**Intermediate** (the queryable models):

| Model | What It Tracks |
|:------|:---------------|
| `int_platform_observability__run_results` | Per-model execution status, timing, rows affected (excludes tests) |
| `int_platform_observability__test_results` | Test pass/fail/warn status and messages |
| `int_platform_observability__model_definitions` | Full DAG: every model's node ID, dependencies, materialization, schema |
| `int_platform_observability__source_freshness_results` | Source freshness check results parsed from dbt artifacts |
| `int_platform_observability__lineage_summary` | Joins run results with the master model registry to trace connector -> staging -> master model lineage |
| `int_platform_observability__md_table_stats` | Physical table inventory from `INFORMATION_SCHEMA` — all schemas and tables in the warehouse |
| `int_platform_observability__source_candidate_map` | Onboarding intelligence — scores unintegrated source tables as `ONBOARD`, `INVESTIGATE SWAP`, or `MONITOR` based on physical inventory + dbt health |
| `int_platform_observability__identity_resolution_stats` | Per-tenant identity resolution rates from `dim_users`: total users, resolved customers, anonymous, resolution % |

**Operational**:
- `obs_platform_status__semantic_readiness` — joins the BSL catalog with latest run results to check if each star schema table's last run succeeded. Returns `is_semantic_layer_ready` per tenant/subject.
- `platform_ops__boring_semantic_layer` — auto-catalogs all `fct_*` and `dim_*` tables from `INFORMATION_SCHEMA`, extracts tenant slug and subject via regex, builds a JSON `semantic_manifest` of column metadata per table

### The DAG as Data

Because every model's dependencies are captured in `depends_on_nodes`, the observability pipeline contains a complete representation of the dbt DAG as queryable data. You can trace any star schema table back through its intermediate model, master model, staging pusher, and source table. Combined with `run_results`, you can answer: "When did `fct_tyrell_corp__orders` last succeed? How long did it take? Which upstream model failed?"

---

## Two-Tier Semantic Layer

The platform serves two fundamentally different query patterns and uses a purpose-built tool for each.

### Tier 1: In-Browser (Single-Table EDA)

A fully client-side semantic layer that runs entirely in the browser with no backend dependency.

**How it works:**
1. User loads a table into DuckDB WASM (from MotherDuck, file upload, or pre-built dataset)
2. The **Semantic Profiler** (`semantic-profiler.ts`) runs DuckDB's `SUMMARIZE` and infers field metadata — column types, cardinality, categorical members, and smart type detection (e.g., VARCHAR columns that are actually dates via `TRY_CAST`)
3. The **Semantic Config** (`semantic-config.ts`) generates a metadata registry: dimensions (categorical/temporal columns with optional transformations), measures (numerical columns with aggregation functions and formulas like `COUNT(DISTINCT session_id)`)
4. The config generates an LLM system prompt containing the full dimension/measure mappings, SQL generation rules, and examples
5. **WebLLM** (`webllm-handler.ts`) loads Qwen 2.5 Coder (3B) via `@mlc-ai/web-llm` and generates SQL from natural language using the semantic prompt, with `temperature: 0.0` for deterministic output
6. The **Semantic Query Validator** (`semantic-query-validator.ts`) checks the generated SQL against the metadata registry — extracts column references, validates them, catches invalid DuckDB functions, and generates correction prompts for retry (up to 3 attempts)
7. DuckDB WASM executes the validated SQL client-side
8. AutoChart renders the visualization

No backend round-trip. Works offline. Instant feedback.

### Tier 2: Backend BSL (Multi-Table OLAP)

Single-table profiling can't express how `fct_ad_performance` joins to `dim_campaigns` on `campaign_id + source_platform`, or that ROAS requires dividing revenue from sessions/orders by spend from ad performance. The backend Boring Semantic Layer defines this cube geometry explicitly.

**Per-tenant YAML configs** (`services/platform-api/semantic_configs/{tenant}.yaml`) define:
- **Dimensions**: columns for grouping/filtering with types (`string`, `date`, `boolean`, `timestamp_epoch`)
- **Measures**: columns for aggregation with default functions (`sum`, `avg`, `count_distinct`)
- **Calculated measures**: derived metrics as SQL expressions (CTR, CPC, CPM, AOV, conversion rate)
- **Joins**: how fact tables relate to dimension tables — column mappings + join type

The BSL definitions are populated from dbt metadata: `model_artifacts__current` provides the DAG and model list, `INFORMATION_SCHEMA` provides column types, and shared column names across tables reveal join paths (`campaign_id` in both `fct_ad_performance` and `dim_campaigns`).

**Current state:** BSL YAML configs are generated and served via `GET /semantic-layer/{tenant}/config`. The query builder that compiles structured semantic queries into SQL is the next phase (see Roadmap).

### Why Two Tiers

| | Tier 1 (In-Browser) | Tier 2 (Backend BSL) |
|:--|:---------------------|:---------------------|
| **Scope** | Single table | Multi-table with joins |
| **Execution** | Client-side DuckDB WASM | Server-side MotherDuck |
| **LLM output** | Raw SQL | Structured `{dimensions, measures, filters, joins}` JSON |
| **Join handling** | N/A | Governed join paths from YAML config |
| **Latency** | Instant (no network) | Network round-trip |
| **Offline** | Yes | No |
| **Use case** | Quick EDA, file uploads | Cross-table analytics, dashboards |

---

## Tenants

| Tenant | Paid Ads | Ecommerce | Analytics |
|:-------|:---------|:----------|:----------|
| Tyrell Corporation | Facebook Ads, Google Ads, Instagram Ads | Shopify | Google Analytics |
| Wayne Enterprises | Bing Ads, Google Ads | BigCommerce | Google Analytics |
| Stark Industries | Facebook Ads, Instagram Ads | WooCommerce | Mixpanel |

Each tenant's configuration lives in `tenants.yaml`, which defines enabled sources, mock data generation parameters, and table-level logic (conversion events, funnel steps, attribution models). All three tenants have fully operational pipelines — 18 star schema tables total (6 per tenant), all passing.

---

## Model Inventory

| Layer | Count | Description |
|:------|:------|:------------|
| Sources | auto-generated | `_sources.yml` shims pointing at raw landed tables |
| Staging | auto-generated | Views that bundle columns into `raw_data_payload` JSON, fire `sync_to_master_hub()` post-hook |
| Master Models | 31 | Multi-tenant sink tables, one per connector object (e.g., `platform_mm__shopify_api_v1_orders`) |
| Hubs + Satellites | 4 | Config history, schema history, key registry, current source configs |
| Platform Ops | ~5 | Master model registry, schema hash registry, BSL catalog, source candidate registry |
| Observability | 9 | Run results, test results, model definitions, lineage, freshness, identity resolution, semantic readiness |
| Intermediate | 20 | Tenant-isolated extractions from JSON (8 Tyrell, 6 Wayne, 6 Stark) |
| Analytics | 18 | Star schema tables (6 per tenant: 4 facts + 2 dimensions) |
| **Total** | **136** | **133 PASS on full `dbt run`** |

---

## Supported Connectors

The connector catalog defines 13 data sources across 3 domains. Each connector maps to a `master_model_id` that determines which master model table receives its data.

### Paid Advertising (7)
| Connector | API Version | Objects |
|:----------|:------------|:--------|
| Facebook Ads | v19 | ads, ad_sets, campaigns, facebook_insights |
| Instagram Ads | v19 | ads, ad_sets, campaigns, facebook_insights |
| Google Ads | v16 | ads, ad_groups, ad_performance, campaigns, customers |
| Bing Ads | v13 | campaigns, ad_groups, ads, account_performance_report |
| LinkedIn Ads | v202401 | campaigns, ad_analytics, ad_analytics_by_campaign |
| Amazon Ads | v3 | sponsored_products campaigns/ad_groups/product_ads |
| TikTok Ads | v1.3 | campaigns, ad_groups, ads, ads_reports_daily |

### Ecommerce (3)
| Connector | API Version | Objects |
|:----------|:------------|:--------|
| Shopify | v1 | orders, products |
| BigCommerce | v3 | orders, products |
| WooCommerce | v3 | orders, products |

### Analytics (3)
| Connector | API Version | Objects |
|:----------|:------------|:--------|
| Google Analytics | GA4 v1 | events |
| Amplitude | v2 | events, users |
| Mixpanel | v2 | events, people |

Facebook Ads and Instagram Ads share the same `master_model_id` (`facebook_ads_api_v1`) because they use the same Meta API. They are differentiated at the intermediate layer via `source_platform` filters.

---

## Key Architectural Decisions

### Why `raw_data_payload` in Master Models
Every master model stores the original source data as a JSON column alongside metadata. This means:
- Intermediate models can extract any field without re-ingesting from the source
- If business logic changes in `tenants.yaml` (e.g., new conversion event), the reporting layer rebuilds from JSON — the logic version changes in the satellite, and `dbt run` applies the new logic to all historical data
- Schema evolution doesn't break existing extractions — new fields appear in the JSON automatically

### Why Tenant Isolation at the Intermediate Layer (Not Earlier)
Master models are multi-tenant by design — they pool all tenants' data for a given source object. Tenant isolation happens at the intermediate layer via `WHERE tenant_slug = '{tenant}'`. This keeps master models simple (one per source object, not one per tenant x source) while ensuring the analytics layer only sees its own data.

### Why Engines and Factories (Not Unified Models)
An earlier implementation attempted unified multi-tenant models (`int_unified_ad_performance`) that used CASE statements across all tenants. This broke tenant isolation and created brittle cross-tenant dependencies. The engine/factory pattern isolates each source's normalization logic in a standalone macro, and the factory simply UNIONs whichever engines a tenant has enabled.

### Why Config-Driven Shell Models
Shell models read their source lists from `dbt_project.yml` vars (populated from `tenants.yaml`) rather than hardcoding platform lists. Adding a new source to a tenant means updating config — no SQL changes needed in the analytics layer.

### Why Schema Fingerprint Routing (Not Naming Conventions)
The `connector_blueprints` registry maps MD5 hashes of physical column structures to master model IDs. This means routing decisions are based on what a table actually looks like, not what it's named. If a connector API changes and produces different columns, the hash changes, and the router either matches a known blueprint or flags the table as unknown — preventing silent data corruption from schema drift.

---

## Repository Structure

```
gata-platform/
|-- app/                                    # Deno/Fresh frontend application
|   |-- islands/                            # Interactive components
|   |   |-- dashboard/smarter_dashboard/    # AI-powered analytics dashboard
|   |   |-- onboarding/                     # Tenant onboarding flow
|   |   +-- app_utils/SemanticProfiler.tsx  # In-browser table profiling
|   |-- utils/
|   |   |-- smarter/                        # Tier 1 semantic layer
|   |   |   |-- dashboard_utils/
|   |   |   |   |-- semantic-config.ts      # Metadata registry + SQL prompt gen
|   |   |   |   |-- semantic-objects.ts     # SemanticReportObj (alias -> SQL)
|   |   |   |   |-- semantic-query-validator.ts
|   |   |   |   +-- slice-object-generator.ts
|   |   |   +-- autovisualization_dashboard/
|   |   |       |-- webllm-handler.ts       # Qwen 2.5 Coder integration
|   |   |       +-- semantic-catalog-generator.ts
|   |   +-- services/
|   |       |-- motherduck-client.ts        # Server-side MotherDuck queries
|   |       +-- table-profiler.ts           # Column metadata extraction
|   +-- routes/                             # Fresh file-based routing
|
|-- warehouse/
|   +-- gata_transformation/                # dbt project
|       |-- models/
|       |   |-- sources/{tenant}/{platform}/         # Auto-generated source shims
|       |   |-- staging/{tenant}/{platform}/         # Auto-generated staging pushers
|       |   |-- platform/
|       |   |   |-- master_models/                   # 31 multi-tenant sinks
|       |   |   |-- hubs/                            # Hub key registry
|       |   |   |-- satellites/                      # Config + schema versioning
|       |   |   |-- ops/                             # BSL catalog, model registry
|       |   |   +-- observability/                   # Run/test/lineage/freshness
|       |   |-- intermediate/{tenant}/{platform}/    # Tenant-isolated extractions
|       |   +-- analytics/{tenant}/                  # Star schema (facts + dims)
|       |-- macros/
|       |   |-- engines/                             # Source normalizers (13 engines)
|       |   |   |-- paid_ads/    (7 engines)
|       |   |   |-- ecommerce/   (3 engines)
|       |   |   +-- analytics/   (3 engines)
|       |   |-- factories/                           # Star schema assemblers (6)
|       |   |   |-- build_fct_ad_performance.sql
|       |   |   |-- build_fct_orders.sql
|       |   |   |-- build_fct_sessions.sql
|       |   |   |-- build_fct_events.sql
|       |   |   |-- build_dim_campaigns.sql
|       |   |   +-- build_dim_users.sql
|       |   |-- onboarding/                          # Push circuit macros
|       |   |   |-- generate_staging_pusher.sql      # Generates staging views
|       |   |   |-- sync_to_master_hub.sql           # MERGE post-hook
|       |   |   +-- sync_to_schema_history.sql       # Schema fingerprint tracking
|       |   |-- utils/                               # JSON extraction, tenant config
|       |   +-- dbt_metadata/                        # on-run-start/end artifact capture
|       |       +-- metadata_ops.sql                 # init + upload macros
|       +-- profiles.yml                             # sandbox (local) + dev (MotherDuck)
|
|-- services/
|   |-- platform-api/                       # FastAPI backend
|   |   |-- main.py                         # /semantic-layer/{tenant} endpoints
|   |   +-- semantic_configs/               # Per-tenant BSL YAML configs
|   +-- mock-data-engine/                   # dlt pipelines + synthetic data generators
|       |-- orchestrator.py                 # MockOrchestrator (Pydantic -> dlt -> DuckDB)
|       |-- sources/{domain}/{platform}/    # Per-connector mock generators
|       +-- schemas/                        # Pydantic schemas (frozen per API version)
|
|-- scripts/
|   |-- onboard_tenant.py                   # Full tenant scaffolding (data + models)
|   +-- initialize_connector_library.py     # Schema fingerprint -> master model registry
|
|-- docs/
|   +-- roadmap.md                          # Frontend integration plan (6 phases)
|
|-- tenants.yaml                            # Tenant configs (source of truth)
|-- supported_connectors.yaml               # Connector catalog (13 sources)
|-- main.py                                 # dlt -> dbt orchestrator
+-- pyproject.toml                          # Python dependencies (uv)
```

---

## Development

### Prerequisites
- Python 3.11+ with [uv](https://github.com/astral-sh/uv) for dependency management
- Deno 1.40+ for the frontend app
- DuckDB CLI (optional, for local exploration)
- MotherDuck account (optional — only needed for `dev` target, set `MOTHERDUCK_TOKEN` in `.env`)

### Running the Pipeline (Sandbox — No External Dependencies)

The sandbox target uses a local DuckDB file. No MotherDuck account needed.

```bash
# Install Python dependencies
uv sync --all-groups

# Initialize connector library (registers schema hashes)
uv run python scripts/initialize_connector_library.py sandbox

# Onboard tenants (generates mock data + scaffolds dbt models)
uv run python scripts/onboard_tenant.py tyrell_corp --target sandbox --days 30
uv run python scripts/onboard_tenant.py wayne_enterprises --target sandbox --days 30
uv run python scripts/onboard_tenant.py stark_industries --target sandbox --days 30

# Run full dbt pipeline
cd warehouse/gata_transformation
uv run dbt run --target sandbox

# Second pass for reporting layer (ensures staging data is populated first)
uv run dbt run --target sandbox --select "models/intermediate models/analytics models/platform/ops/platform_ops__boring_semantic_layer.sql"
```

### Running the Pipeline (MotherDuck)

```bash
# Set MOTHERDUCK_TOKEN in .env at project root, then:
cd warehouse/gata_transformation
uv run --env-file ../../.env dbt run --target dev
```

### Running the Frontend
```bash
cd app
deno task start
```

### Running the API
```bash
cd services/platform-api
uvicorn main:app --reload
```

### Adding a New Tenant
1. Add tenant config to `tenants.yaml` with enabled sources and table-level logic
2. Run `scripts/onboard_tenant.py <slug> --target sandbox` to scaffold source/staging/intermediate models
3. Shell models in `models/analytics/{slug}/` are auto-generated based on config
4. Run `dbt run` to materialize the full pipeline

### Adding a New Connector
1. Add connector definition to `supported_connectors.yaml`
2. Create Pydantic schema in `services/mock-data-engine/schemas/`
3. Create mock data generator in `services/mock-data-engine/sources/{domain}/`
4. Create master model: `platform_mm__{master_model_id}_{object}.sql`
5. Create engine macro in `macros/engines/{domain}/`
6. Add engine to the relevant factory's `engine_map`

---

## Roadmap

The backend transformation pipeline is complete. The next phase connects the Deno Fresh frontend to the star schema via the BSL semantic layer and adds governed query execution.

### Phase 1: Clean Up Legacy Dashboards
Remove the Rill Developer dashboard service (stale, not compatible with current star schema) and clean up related code paths.

### Phase 2: Query Builder + API Endpoints
Build a query compiler in `services/platform-api/query_builder.py` that translates structured semantic queries into SQL using BSL YAML configs. The frontend's LLM (Qwen 2.5 Coder) will output `{dimensions, measures, filters, joins}` JSON instead of raw SQL — the backend validates field names against the BSL config, enforces join paths, and returns deterministic results.

New endpoints:
- `POST /semantic-layer/{tenant}/query` — accepts structured query, returns compiled SQL + results
- `GET /semantic-layer/{tenant}/models` — lists available star schema tables with their dimensions/measures
- `GET /semantic-layer/{tenant}/models/{model}` — full model config (dimensions, measures, calculated measures, join paths)

### Phase 3: Tenant-Scoped Observability
Extend the existing observability models to filter by tenant (model names encode tenant slugs). Expose per-tenant pipeline health via API — last run time, pass/fail counts, execution times, identity resolution stats. Runs in parallel with Phase 2.

### Phase 4: Frontend Integration
Wire the Deno Fresh dashboard to consume BSL configs from the API instead of static JSON files. Update WebLLM to output structured semantic queries. Build an observability dashboard showing per-tenant pipeline health. Key changes:
- Dashboard loads BSL config on mount (replaces hardcoded metadata)
- WebLLM generates structured queries validated against BSL schema
- Single-table queries execute locally via DuckDB WASM; multi-table joins route to the backend API
- New pipeline health dashboard surfaces observability data per tenant

### Phase 5: DuckDB WASM Local Enrichment
Enable users to upload CSV/Parquet files, load them into DuckDB WASM alongside star schema data fetched from the backend, and JOIN local files with platform data for ad-hoc enrichment. The semantic profiler auto-generates metadata for uploaded tables.

### Phase 6: dlt Lineage Integration
Link dlt load IDs through the observability models so each run result traces back to a specific data load. Surface end-to-end lineage (dlt load -> staging -> intermediate -> analytics) and use dlt schema metadata for automated drift detection.

**Phases 2 and 3 can run in parallel.** Phase 4 depends on both. Phases 5 and 6 are additive.

See [docs/roadmap.md](docs/roadmap.md) for the full implementation plan with file-level detail.

---

## Documentation

- [Roadmap](docs/roadmap.md) — Full implementation plan for frontend integration, query builder, observability, and enrichment features
