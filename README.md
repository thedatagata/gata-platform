# GATA Platform

**Multi-Tenant Analytics Platform with Automated Onboarding, Source-Agnostic Star Schemas, and a Two-Tier Semantic Layer**

**Status:** Alpha — Backend transformation pipeline operational, frontend integration in progress  
**Stack:** dlt · dbt · DuckDB / MotherDuck · Deno/Fresh · FastAPI · Boring Semantic Layer  

---

## What This Project Is

GATA Platform is a portfolio project that demonstrates how a single analytics engineer can build a production-grade, multi-tenant data platform that:

1. **Onboards tenants automatically** — a new customer selects their data sources, and the platform scaffolds ingestion, transformation, and reporting without manual SQL.
2. **Produces the same star schema regardless of source mix** — whether a tenant uses Shopify + Facebook Ads or BigCommerce + Bing Ads, the analytics layer outputs identical fact and dimension tables through engine/factory macros.
3. **Preserves raw data for logic replay** — every record carries its original JSON payload alongside extracted fields, so when a tenant changes their conversion event definition or UTM mapping in their config, we can rebuild their reporting layer from the raw data without re-ingesting.
4. **Exposes analytics through a two-tier semantic layer** — lightweight in-browser text-to-SQL for single-table EDA (DuckDB WASM + Qwen 2.5 Coder), and a backend Boring Semantic Layer for governed multi-table OLAP queries served via FastAPI and MCP.

The project uses fictional tenants (Tyrell Corporation, Wayne Enterprises, Stark Industries) with synthetic data generated by mock data engines. No proprietary data or client IP is involved.

---

## Architecture Overview

### The Data Flow

```
┌─────────────────────────────────────────────────────────────────────────┐
│                         INGESTION (dlt)                                 │
│  Mock Data Generators → Pydantic Schemas → DuckDB/MotherDuck Landing   │
└────────────────────────────────┬────────────────────────────────────────┘
                                 │
┌────────────────────────────────▼────────────────────────────────────────┐
│                      TRANSFORMATION (dbt)                               │
│                                                                         │
│  sources/{tenant}/{platform}/     ← Source shims (_sources.yml)         │
│       │                                                                 │
│  staging/{tenant}/{platform}/     ← Metadata enrichment + JSON bundle   │
│       │                              post-hook: sync_to_master_hub()    │
│       │                                                                 │
│  platform/master_models/          ← Multi-tenant sinks (31 models)      │
│       │                              One per connector object           │
│       │                              Contains raw_data_payload JSON     │
│       │                                                                 │
│  intermediate/{tenant}/{platform}/ ← Tenant isolation + field extraction│
│       │                              Filters by tenant_slug             │
│       │                              Extracts from raw_data_payload     │
│       │                              Applies tenant-specific logic      │
│       │                                                                 │
│  analytics/{tenant}/              ← Star schema (facts + dimensions)    │
│                                      Engine macros normalize sources    │
│                                      Factory macros union by source mix │
└────────────────────────────────┬────────────────────────────────────────┘
                                 │
┌────────────────────────────────▼────────────────────────────────────────┐
│                      SEMANTIC LAYER (Two-Tier)                          │
│                                                                         │
│  Tier 1: In-Browser (existing)                                          │
│    DuckDB WASM loads single table → auto-profiler generates metadata    │
│    → WebLLM (Qwen 2.5 Coder 3B) generates SQL from natural language    │
│    → SemanticReportObj executes against WASM instance                   │
│    → AutoChart renders visualization                                    │
│    Purpose: Single-table EDA, no backend dependency                     │
│                                                                         │
│  Tier 2: Backend BSL (in progress)                                      │
│    Post-dbt-run script introspects analytics tables                     │
│    → Generates BSL definitions (dimensions, measures, join paths)       │
│    → FastAPI serves per-tenant semantic endpoints                       │
│    → MCP endpoint for AI agent consumption                              │
│    → MotherDuck AI features for complex cross-table queries             │
│    Purpose: Multi-table OLAP, governed join paths, deterministic SQL    │
└─────────────────────────────────────────────────────────────────────────┘
```

### Shell and Engine Architecture

The reporting layer uses a **Shell and Engine** pattern to decouple source-specific logic from the star schema contract:

- **Engines** (`macros/engines/`) are dbt macros that know how to read a specific source's intermediate model and normalize it to a canonical schema. There are 13 engines across 3 domains: 7 paid ads, 3 ecommerce, 3 analytics.
- **Factories** (`macros/factories/`) are dbt macros that accept a tenant slug and a list of enabled sources, look up the correct engine for each source, and UNION ALL the results. If a source isn't in the tenant's mix, it's simply skipped.
- **Shell Models** (`models/analytics/{tenant}/`) are thin SQL files that call the appropriate factory with the tenant's specific source list from `tenants.yaml`. These are the only models that need to be created per tenant — the engines and factories are shared.

This means `fct_tyrell_corp__ad_performance.sql` and `fct_wayne_enterprises__ad_performance.sql` both produce the same column contract (`tenant_slug, source_platform, report_date, campaign_id, ad_group_id, ad_id, spend, impressions, clicks, conversions`) despite one pulling from Facebook + Google + Instagram and the other from Bing + Google.

---

## Repository Structure

```
gata-platform/
├── app/                                    # Deno/Fresh frontend application
│   ├── islands/                            # Interactive components
│   │   ├── dashboard/smarter_dashboard/    # AI-powered analytics dashboard
│   │   ├── onboarding/                     # Tenant onboarding flow
│   │   └── app_utils/SemanticProfiler.tsx  # In-browser table profiling
│   ├── utils/
│   │   ├── smarter/                        # Tier 1 semantic layer
│   │   │   ├── dashboard_utils/
│   │   │   │   ├── semantic-config.ts      # Metadata registry + SQL prompt gen
│   │   │   │   ├── semantic-objects.ts     # SemanticReportObj (alias→SQL)
│   │   │   │   ├── semantic-query-validator.ts  # LLM output validation
│   │   │   │   └── slice-object-generator.ts    # WebDataRocks pivot config
│   │   │   └── autovisualization_dashboard/
│   │   │       ├── webllm-handler.ts       # Qwen 2.5 Coder integration
│   │   │       └── semantic-catalog-generator.ts
│   │   ├── system/semantic-profiler.ts     # Auto-profile via SUMMARIZE
│   │   └── services/
│   │       ├── motherduck-client.ts        # Server-side MotherDuck queries
│   │       └── table-profiler.ts           # Column metadata extraction
│   ├── static/smarter/                     # Pre-built semantic metadata JSONs
│   └── routes/                             # Fresh file-based routing
│
├── warehouse/
│   └── gata_transformation/                # The dbt project
│       ├── models/
│       │   ├── sources/{tenant}/{platform}/         # Source shims
│       │   ├── staging/{tenant}/{platform}/         # Pushers → master models
│       │   ├── platform/
│       │   │   ├── master_models/                   # 31 multi-tenant sinks
│       │   │   ├── hubs/                            # Key registry
│       │   │   ├── satellites/                      # Schema + config history
│       │   │   ├── ops/                             # Operational metadata
│       │   │   └── observability/                   # dbt run/test artifacts
│       │   ├── intermediate/{tenant}/{platform}/    # Tenant-isolated extractions
│       │   └── analytics/{tenant}/                  # Star schema (facts + dims)
│       ├── macros/
│       │   ├── engines/                             # Source normalizers
│       │   │   ├── paid_ads/    (7 engines)
│       │   │   ├── ecommerce/   (3 engines)
│       │   │   └── analytics/   (3 engines)
│       │   ├── factories/                           # Star schema assemblers
│       │   │   ├── build_fct_ad_performance.sql
│       │   │   ├── build_fct_orders.sql
│       │   │   ├── build_fct_sessions.sql
│       │   │   └── build_dim_campaigns.sql
│       │   ├── onboarding/                          # Push circuit macros
│       │   │   ├── generate_staging_pusher.sql
│       │   │   ├── sync_to_master_hub.sql
│       │   │   └── sync_to_schema_history.sql
│       │   ├── utils/                               # JSON extraction, tenant config
│       │   └── dbt_metadata/                        # Artifact capture (on-run-end)
│       └── profiles.yml                             # sandbox (local) + dev (MotherDuck)
│
├── services/
│   ├── platform-api/                       # FastAPI backend
│   │   └── main.py                         # /semantic-layer/{tenant}, /update
│   ├── mock-data-engine/                   # dlt pipelines + synthetic data
│   └── rill-dashboard/                     # Rill Developer metrics (optional)
│
├── scripts/                                # Automation scripts
│   └── onboard_tenant.py                   # Scaffolds dbt models for new tenants
│
├── docs/
│   ├── architecture/
│   │   ├── registry_driven_push.md         # Push architecture deep dive
│   │   └── semantic_layer_strategy.md      # Two-tier semantic layer design
│   └── roadmap.md                          # Implementation phases + status
│
├── tenants.yaml                            # Tenant configs (source of truth)
├── supported_connectors.yaml               # Connector catalog (13 sources)
├── main.py                                 # dlt→dbt runner (create_runner)
└── pyproject.toml                          # Python dependencies (uv)
```

---

## Supported Connectors

The platform's connector catalog defines 13 data sources across 3 domains. Each connector maps to a `master_model_id` that determines which master model sinks receive its data.

### Paid Advertising (7)
| Connector | Version | Master Model ID | Objects |
|:----------|:--------|:----------------|:--------|
| Facebook Ads | v19 | `facebook_ads_api_v1` | ads, ad_sets, campaigns, facebook_insights |
| Instagram Ads | v19 | `facebook_ads_api_v1` | ads, ad_sets, campaigns, facebook_insights |
| Google Ads | v16 | `google_ads_api_v1` | ads, ad_groups, ad_performance, campaigns, customers |
| Bing Ads | v13 | `bing_ads_api_v1` | campaigns, ad_groups, ads, account_performance_report |
| LinkedIn Ads | v202401 | `linkedin_ads_api_v1` | campaigns, ad_analytics, ad_analytics_by_campaign |
| Amazon Ads | v3 | `amazon_ads_api_v1` | sponsored_products_campaigns/ad_groups/product_ads |
| TikTok Ads | v1.3 | `tiktok_ads_api_v1` | campaigns, ad_groups, ads, ads_reports_daily |

### Ecommerce (3)
| Connector | Version | Master Model ID | Objects |
|:----------|:--------|:----------------|:--------|
| Shopify | v1 | `shopify_api_v1` | orders, products |
| BigCommerce | v3 | `bigcommerce_api_v1` | orders, products |
| WooCommerce | v3 | `woocommerce_api_v1` | orders, products |

### Analytics / Product Analytics (3)
| Connector | Version | Master Model ID | Objects |
|:----------|:--------|:----------------|:--------|
| Google Analytics | ga4_v1 | `google_analytics_api_v1` | events |
| Amplitude | v2 | `amplitude_api_v1` | events, users |
| Mixpanel | v2 | `mixpanel_api_v1` | events, people |

Note: Facebook Ads and Instagram Ads share the same `master_model_id` (`facebook_ads_api_v1`) because they use the same Meta API. They are differentiated at the intermediate layer via `source_platform` filters.

---

## Tenants

| Tenant | Status | Paid Ads | Ecommerce | Analytics |
|:-------|:-------|:---------|:----------|:----------|
| Tyrell Corporation | Active | Facebook, Google, Instagram | Shopify | Google Analytics |
| Wayne Enterprises | Onboarding | Bing, Google | BigCommerce | Google Analytics |
| Stark Industries | Onboarding | Facebook, Instagram | WooCommerce | Mixpanel |

Each tenant's configuration lives in `tenants.yaml`, which defines enabled sources, mock data generation parameters, and table-level logic (conversion events, funnel steps, attribution models, etc.).

---

## Engine and Factory Contracts

### Paid Ads — Canonical Schema
All 7 paid ads engines normalize to:
```
tenant_slug | source_platform | report_date | campaign_id | ad_group_id | ad_id | spend | impressions | clicks | conversions
```
Sources that lack granularity (e.g., Bing's account-level report has no `campaign_id`) emit `CAST(NULL AS VARCHAR)` for missing columns.

### Ecommerce — Canonical Schema
All 3 ecommerce engines normalize to:
```
tenant_slug | source_platform | order_id | order_date | total_price | currency | financial_status | customer_email | customer_id | line_items_json
```

### Analytics — Canonical Schema
All 3 analytics engines produce sessionized output:
```
tenant_slug | source_platform | session_id | user_pseudo_id | user_id | session_start_ts | session_end_ts | session_duration_seconds | events_in_session | traffic_source | traffic_medium | traffic_campaign | geo_country | device_category | is_conversion_session | session_revenue | transaction_id
```
Google Analytics and Mixpanel perform 30-minute inactivity sessionization. Amplitude uses native `session_id`. All accept a `conversion_events` list parameter for the `is_conversion_session` flag.

---

## Key Architectural Decisions

### Why raw_data_payload in Master Models
Every master model stores the original source data as a JSON column alongside metadata (`tenant_slug`, `source_platform`, `tenant_skey`, `loaded_at`, `source_schema_hash`). This means:
- Intermediate models can extract any field without re-ingesting
- If `tenants.yaml` logic changes (e.g., new conversion event), we rebuild from the JSON
- Schema evolution doesn't break existing extractions — new fields appear in the JSON automatically

### Why Tenant Isolation at the Intermediate Layer (Not Earlier)
Master models are multi-tenant by design — they pool all tenants' data for a given source object. Tenant isolation happens at the intermediate layer via `WHERE tenant_slug = '{tenant}'`. This keeps master models simple (one per source object, not one per tenant × source) while ensuring the analytics layer only sees its own data. A frontend app with client logins needs this boundary.

### Why Engines and Factories (Not Unified Models)
An earlier implementation attempted unified multi-tenant models (`int_unified_ad_performance`) that used CASE statements across all tenants. This broke tenant isolation and created brittle cross-tenant dependencies. The engine/factory pattern isolates each source's normalization logic in a standalone macro, and the factory simply UNIONs whichever engines a tenant has enabled.

---

## Development

### Prerequisites
- Python 3.11+ with `uv` for dependency management
- Deno 1.40+ for the frontend app
- DuckDB CLI (optional, for local exploration)
- MotherDuck account (for dev target — set `MOTHERDUCK_TOKEN` in `.env`)

### Running the Pipeline
```bash
# Install Python deps
uv sync --all-groups

# Run ingestion + transformation
python main.py

# Or run dbt directly
cd warehouse/gata_transformation
dbt run --target dev          # MotherDuck
dbt run --target sandbox      # Local DuckDB file
```

### Running the Frontend
```bash
cd app
deno task start
```

### Running the API
```bash
cd services/platform-api
uvicorn main:app --reload
```

### Adding a New Tenant
1. Add tenant config to `tenants.yaml` with enabled sources
2. Run `scripts/onboard_tenant.py` to scaffold source/staging/intermediate models
3. Create analytics shell models in `models/analytics/{tenant_slug}/`
4. Run `dbt run` to materialize

### Adding a New Connector
1. Add connector definition to `supported_connectors.yaml`
2. Create Pydantic schema in `services/mock-data-engine/schemas/`
3. Create mock data generator
4. Create master model: `platform_mm__{master_model_id}_{object}.sql`
5. Create engine macro in `macros/engines/{domain}/`
6. Add engine to the relevant factory's `engine_map`

---

## Documentation

- [Architecture: Registry-Driven Push](docs/architecture/registry_driven_push.md) — How data flows from landing to master models
- [Architecture: Semantic Layer Strategy](docs/architecture/semantic_layer_strategy.md) — Two-tier semantic layer design
- [Roadmap](docs/roadmap.md) — Implementation phases, current status, and next steps
