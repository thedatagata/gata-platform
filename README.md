# GATA Platform

**Multi-Tenant Analytics Platform with Automated Onboarding, Source-Agnostic Star Schemas, and a Two-Tier Semantic Layer**

**Status:** Alpha — Backend transformation pipeline fully operational (136 models, 133 PASS), frontend integration in progress
**Stack:** dlt / dbt / DuckDB / MotherDuck / Deno Fresh / FastAPI / Boring Semantic Layer

---

## What This Project Is

GATA Platform is a multi-tenant data platform that demonstrates how a single analytics engineer can architect and build a production-grade system covering ingestion, transformation, governance, and analytics serving. The project solves a real problem in the marketing analytics space: businesses use different combinations of ad platforms, ecommerce tools, and analytics products, but they all need the same reporting outputs.

The platform:

1. **Onboards tenants automatically** — a new customer selects their data sources, and the platform scaffolds ingestion, staging, and reporting models without manual SQL. The onboarding script generates dbt models, registers schema hashes, and wires up the push circuit.
2. **Produces identical star schemas regardless of source mix** — whether a tenant uses Shopify + Facebook Ads or BigCommerce + Bing Ads, the analytics layer outputs the same fact and dimension tables through an engine/factory macro architecture. 13 source-specific engines normalize to 6 canonical star schema tables per tenant.
3. **Preserves raw data for logic replay** — every record carries its original JSON payload alongside extracted fields, so when business logic changes (conversion event definitions, attribution models, UTM mappings), the reporting layer can be rebuilt from raw data without re-ingesting.
4. **Exposes analytics through a two-tier semantic layer** — lightweight in-browser text-to-SQL for single-table exploration (DuckDB WASM + Qwen 2.5 Coder), and a backend Boring Semantic Layer for governed multi-table OLAP queries served via FastAPI.

The project uses fictional tenants (Tyrell Corporation, Wayne Enterprises, Stark Industries) with synthetic data generated by mock data engines. No proprietary data or client IP is involved.

---

## Architecture Overview

### The Data Flow

```
+-----------------------------------------------------------------------+
|                         INGESTION (dlt)                                |
|  Mock Data Generators -> Pydantic Schemas -> DuckDB/MotherDuck Landing |
+-----------------------------------+-----------------------------------+
                                    |
+-----------------------------------v-----------------------------------+
|                      TRANSFORMATION (dbt)                              |
|                                                                        |
|  sources/{tenant}/{platform}/       <- Source shims (_sources.yml)      |
|       |                                                                |
|  staging/{tenant}/{platform}/       <- JSON bundling + metadata        |
|       |                                post-hook: sync_to_master_hub() |
|       |                                                                |
|  platform/master_models/            <- Multi-tenant sinks (31 models)  |
|       |                                One per connector object         |
|       |                                Contains raw_data_payload JSON   |
|       |                                                                |
|  intermediate/{tenant}/{platform}/  <- Tenant isolation + extraction   |
|       |                                Filters by tenant_slug          |
|       |                                Typed fields from JSON           |
|       |                                                                |
|  analytics/{tenant}/                <- Star schema (facts + dims)      |
|                                        Engine macros normalize sources |
|                                        Factory macros union by mix     |
+-----------------------------------+-----------------------------------+
                                    |
+-----------------------------------v-----------------------------------+
|                      SEMANTIC LAYER (Two-Tier)                         |
|                                                                        |
|  Tier 1: In-Browser                                                    |
|    DuckDB WASM loads single table -> auto-profiler generates metadata  |
|    -> WebLLM (Qwen 2.5 Coder 3B) generates SQL from natural language   |
|    -> SemanticReportObj executes against WASM instance                  |
|    -> AutoChart renders visualization                                  |
|    Purpose: Single-table EDA, no backend dependency                    |
|                                                                        |
|  Tier 2: Backend BSL (in progress)                                     |
|    Per-tenant YAML configs define dimensions, measures, join paths     |
|    -> FastAPI serves semantic endpoints per tenant                      |
|    -> Query builder compiles structured queries to SQL                  |
|    -> MotherDuck executes governed multi-table joins                    |
|    Purpose: Multi-table OLAP, governed join paths, deterministic SQL   |
+------------------------------------------------------------------------+
```

### Shell and Engine Architecture

The reporting layer uses a **Shell and Engine** pattern to decouple source-specific logic from the star schema contract:

- **Engines** (`macros/engines/`) are dbt macros that know how to read a specific source's intermediate model and normalize it to a canonical schema. There are 13 engines across 3 domains: 7 paid ads, 3 ecommerce, 3 analytics.
- **Factories** (`macros/factories/`) are dbt macros that accept a tenant slug and a list of enabled sources, look up the correct engine for each source, and UNION ALL the results. There are 6 factories — one per star schema table.
- **Shell Models** (`models/analytics/{tenant}/`) are thin SQL files that call the appropriate factory with the tenant's source list from config. These are the only models created per tenant — engines and factories are shared.

This means `fct_tyrell_corp__ad_performance.sql` and `fct_wayne_enterprises__ad_performance.sql` both produce the same column contract (`tenant_slug, source_platform, report_date, campaign_id, ad_group_id, ad_id, spend, impressions, clicks, conversions`) despite pulling from completely different source platforms.

---

## Tenants

| Tenant | Paid Ads | Ecommerce | Analytics |
|:-------|:---------|:----------|:----------|
| Tyrell Corporation | Facebook Ads, Google Ads, Instagram Ads | Shopify | Google Analytics |
| Wayne Enterprises | Bing Ads, Google Ads | BigCommerce | Google Analytics |
| Stark Industries | Facebook Ads, Instagram Ads | WooCommerce | Mixpanel |

Each tenant's configuration lives in `tenants.yaml`, which defines enabled sources, mock data generation parameters, and table-level logic (conversion events, funnel steps, attribution models). All three tenants have fully operational pipelines — 18 star schema tables total (6 per tenant), all passing.

---

## Model Inventory

| Layer | Count | Description |
|:------|:------|:------------|
| Sources | auto-generated | `_sources.yml` shims pointing at raw landed tables |
| Staging | auto-generated | Views that bundle columns into `raw_data_payload` JSON, fire `sync_to_master_hub()` post-hook |
| Master Models | 31 | Multi-tenant sink tables, one per connector object (e.g., `platform_mm__shopify_api_v1_orders`) |
| Platform Governance | ~10 | Config history, key registry, source candidate registry, schema tracking |
| Observability | 9 | Run results, test results, lineage, identity resolution stats, semantic readiness |
| Intermediate | 20 | Tenant-isolated extractions from JSON (8 Tyrell, 6 Wayne, 6 Stark) |
| Analytics | 18 | Star schema tables (6 per tenant: 4 facts + 2 dimensions) |
| Boring Semantic Layer | 1 | Auto-catalogs all star schema tables with column metadata via `INFORMATION_SCHEMA` |
| **Total** | **136** | **133 PASS on full `dbt run`** |

### Star Schema Tables (per tenant)

| Model | Description |
|:------|:------------|
| `fct_{slug}__ad_performance` | UNION ALL of ad engines — spend, impressions, clicks, conversions by ad/date |
| `fct_{slug}__orders` | Ecommerce orders — order_id, total_price, currency, customer info, line items |
| `fct_{slug}__sessions` | Sessionized analytics events — 30-min window, attribution, conversion flags |
| `fct_{slug}__events` | Raw analytics event stream with attribution and optional ecommerce data |
| `dim_{slug}__campaigns` | Campaign dimension across ad platforms — campaign_id, name, status |
| `dim_{slug}__users` | Unified user dimension with identity resolution across analytics + ecommerce |

---

## Supported Connectors

The connector catalog defines 13 data sources across 3 domains. Each connector maps to a `master_model_id` that determines which master model table receives its data.

### Paid Advertising (7)
| Connector | API Version | Objects |
|:----------|:------------|:--------|
| Facebook Ads | v19 | ads, ad_sets, campaigns, facebook_insights |
| Instagram Ads | v19 | ads, ad_sets, campaigns, facebook_insights |
| Google Ads | v16 | ads, ad_groups, ad_performance, campaigns, customers |
| Bing Ads | v13 | campaigns, ad_groups, ads, account_performance_report |
| LinkedIn Ads | v202401 | campaigns, ad_analytics, ad_analytics_by_campaign |
| Amazon Ads | v3 | sponsored_products campaigns/ad_groups/product_ads |
| TikTok Ads | v1.3 | campaigns, ad_groups, ads, ads_reports_daily |

### Ecommerce (3)
| Connector | API Version | Objects |
|:----------|:------------|:--------|
| Shopify | v1 | orders, products |
| BigCommerce | v3 | orders, products |
| WooCommerce | v3 | orders, products |

### Analytics (3)
| Connector | API Version | Objects |
|:----------|:------------|:--------|
| Google Analytics | GA4 v1 | events |
| Amplitude | v2 | events, users |
| Mixpanel | v2 | events, people |

Facebook Ads and Instagram Ads share the same `master_model_id` (`facebook_ads_api_v1`) because they use the same Meta API. They are differentiated at the intermediate layer via `source_platform` filters.

---

## Key Architectural Decisions

### Why `raw_data_payload` in Master Models
Every master model stores the original source data as a JSON column alongside metadata (`tenant_slug`, `source_platform`, `tenant_skey`, `loaded_at`, `source_schema_hash`). This means:
- Intermediate models can extract any field without re-ingesting from the source
- If business logic changes in `tenants.yaml` (e.g., new conversion event), the reporting layer rebuilds from JSON
- Schema evolution doesn't break existing extractions — new fields appear in the JSON automatically

### Why Tenant Isolation at the Intermediate Layer (Not Earlier)
Master models are multi-tenant by design — they pool all tenants' data for a given source object. Tenant isolation happens at the intermediate layer via `WHERE tenant_slug = '{tenant}'`. This keeps master models simple (one per source object, not one per tenant x source) while ensuring the analytics layer only sees its own data.

### Why Engines and Factories (Not Unified Models)
An earlier implementation attempted unified multi-tenant models (`int_unified_ad_performance`) that used CASE statements across all tenants. This broke tenant isolation and created brittle cross-tenant dependencies. The engine/factory pattern isolates each source's normalization logic in a standalone macro, and the factory simply UNIONs whichever engines a tenant has enabled.

### Why Config-Driven Shell Models
Shell models read their source lists from `dbt_project.yml` vars (populated from `tenants.yaml`) rather than hardcoding platform lists. Adding a new source to a tenant means updating config — no SQL changes needed in the analytics layer.

---

## Repository Structure

```
gata-platform/
|-- app/                                    # Deno/Fresh frontend application
|   |-- islands/                            # Interactive components
|   |   |-- dashboard/smarter_dashboard/    # AI-powered analytics dashboard
|   |   |-- onboarding/                     # Tenant onboarding flow
|   |   +-- app_utils/SemanticProfiler.tsx  # In-browser table profiling
|   |-- utils/
|   |   |-- smarter/                        # Tier 1 semantic layer
|   |   |   |-- dashboard_utils/
|   |   |   |   |-- semantic-config.ts      # Metadata registry + SQL prompt gen
|   |   |   |   |-- semantic-objects.ts     # SemanticReportObj (alias -> SQL)
|   |   |   |   |-- semantic-query-validator.ts
|   |   |   |   +-- slice-object-generator.ts
|   |   |   +-- autovisualization_dashboard/
|   |   |       |-- webllm-handler.ts       # Qwen 2.5 Coder integration
|   |   |       +-- semantic-catalog-generator.ts
|   |   +-- services/
|   |       |-- motherduck-client.ts        # Server-side MotherDuck queries
|   |       +-- table-profiler.ts           # Column metadata extraction
|   +-- routes/                             # Fresh file-based routing
|
|-- warehouse/
|   +-- gata_transformation/                # dbt project
|       |-- models/
|       |   |-- sources/{tenant}/{platform}/         # Auto-generated source shims
|       |   |-- staging/{tenant}/{platform}/         # Auto-generated staging pushers
|       |   |-- platform/
|       |   |   |-- master_models/                   # 31 multi-tenant sinks
|       |   |   |-- hubs/                            # Key registry
|       |   |   |-- satellites/                      # Schema + config history
|       |   |   |-- ops/                             # BSL + operational metadata
|       |   |   +-- observability/                   # Run/test artifacts + lineage
|       |   |-- intermediate/{tenant}/{platform}/    # Tenant-isolated extractions
|       |   +-- analytics/{tenant}/                  # Star schema (facts + dims)
|       |-- macros/
|       |   |-- engines/                             # Source normalizers (13 engines)
|       |   |   |-- paid_ads/    (7 engines)
|       |   |   |-- ecommerce/   (3 engines)
|       |   |   +-- analytics/   (3 engines)
|       |   |-- factories/                           # Star schema assemblers (6)
|       |   |   |-- build_fct_ad_performance.sql
|       |   |   |-- build_fct_orders.sql
|       |   |   |-- build_fct_sessions.sql
|       |   |   |-- build_fct_events.sql
|       |   |   |-- build_dim_campaigns.sql
|       |   |   +-- build_dim_users.sql
|       |   |-- onboarding/                          # Push circuit macros
|       |   |   |-- generate_staging_pusher.sql
|       |   |   |-- sync_to_master_hub.sql
|       |   |   +-- sync_to_schema_history.sql
|       |   |-- utils/                               # JSON extraction, tenant config
|       |   +-- dbt_metadata/                        # Artifact capture (on-run-end)
|       +-- profiles.yml                             # sandbox (local) + dev (MotherDuck)
|
|-- services/
|   |-- platform-api/                       # FastAPI backend
|   |   |-- main.py                         # /semantic-layer/{tenant} endpoints
|   |   +-- semantic_configs/               # Per-tenant BSL YAML configs
|   +-- mock-data-engine/                   # dlt pipelines + synthetic data generators
|
|-- scripts/                                # Automation scripts
|   |-- onboard_tenant.py                   # Scaffolds full dbt model set for a tenant
|   +-- initialize_connector_library.py     # Registers schema hashes -> master models
|
|-- tenants.yaml                            # Tenant configs (source of truth)
|-- supported_connectors.yaml               # Connector catalog (13 sources)
|-- main.py                                 # dlt -> dbt orchestrator
+-- pyproject.toml                          # Python dependencies (uv)
```

---

## Development

### Prerequisites
- Python 3.11+ with [uv](https://github.com/astral-sh/uv) for dependency management
- Deno 1.40+ for the frontend app
- DuckDB CLI (optional, for local exploration)
- MotherDuck account (optional — only needed for `dev` target, set `MOTHERDUCK_TOKEN` in `.env`)

### Running the Pipeline (Sandbox — No External Dependencies)

The sandbox target uses a local DuckDB file. No MotherDuck account needed.

```bash
# Install Python dependencies
uv sync --all-groups

# Initialize connector library (registers schema hashes)
uv run python scripts/initialize_connector_library.py sandbox

# Onboard tenants (generates mock data + scaffolds dbt models)
uv run python scripts/onboard_tenant.py tyrell_corp --target sandbox --days 30
uv run python scripts/onboard_tenant.py wayne_enterprises --target sandbox --days 30
uv run python scripts/onboard_tenant.py stark_industries --target sandbox --days 30

# Run full dbt pipeline
cd warehouse/gata_transformation
uv run dbt run --target sandbox

# Second pass for reporting layer (ensures staging data is populated first)
uv run dbt run --target sandbox --select "models/intermediate models/analytics models/platform/ops/platform_ops__boring_semantic_layer.sql"
```

### Running the Pipeline (MotherDuck)

```bash
# Set MOTHERDUCK_TOKEN in .env at project root, then:
cd warehouse/gata_transformation
uv run --env-file ../../.env dbt run --target dev
```

### Running the Frontend
```bash
cd app
deno task start
```

### Running the API
```bash
cd services/platform-api
uvicorn main:app --reload
```

### Adding a New Tenant
1. Add tenant config to `tenants.yaml` with enabled sources and table-level logic
2. Run `scripts/onboard_tenant.py <slug> --target sandbox` to scaffold source/staging/intermediate models
3. Shell models in `models/analytics/{slug}/` are auto-generated based on config
4. Run `dbt run` to materialize the full pipeline

### Adding a New Connector
1. Add connector definition to `supported_connectors.yaml`
2. Create Pydantic schema in `services/mock-data-engine/schemas/`
3. Create mock data generator in `services/mock-data-engine/sources/{domain}/`
4. Create master model: `platform_mm__{master_model_id}_{object}.sql`
5. Create engine macro in `macros/engines/{domain}/`
6. Add engine to the relevant factory's `engine_map`

---

## Roadmap

The backend transformation pipeline is complete. The next phase connects the Deno Fresh frontend to the star schema via the BSL semantic layer and adds governed query execution.

### Phase 1: Clean Up Legacy Dashboards
Remove the Rill Developer dashboard service (stale, not compatible with current star schema) and clean up related code paths.

### Phase 2: Query Builder + API Endpoints
Build a query compiler in `services/platform-api/query_builder.py` that translates structured semantic queries into SQL using BSL YAML configs. The frontend's LLM (Qwen 2.5 Coder) will output `{dimensions, measures, filters, joins}` JSON instead of raw SQL — the backend validates field names against the BSL config, enforces join paths, and returns deterministic results.

New endpoints:
- `POST /semantic-layer/{tenant}/query` — accepts structured query, returns compiled SQL + results
- `GET /semantic-layer/{tenant}/models` — lists available star schema tables with their dimensions/measures
- `GET /semantic-layer/{tenant}/models/{model}` — full model config (dimensions, measures, calculated measures, join paths)

### Phase 3: Tenant-Scoped Observability
Extend the existing observability models to filter by tenant (model names encode tenant slugs). Expose per-tenant pipeline health via API — last run time, pass/fail counts, execution times, identity resolution stats. Runs in parallel with Phase 2.

### Phase 4: Frontend Integration
Wire the Deno Fresh dashboard to consume BSL configs from the API instead of static JSON files. Update WebLLM to output structured semantic queries. Build an observability dashboard showing per-tenant pipeline health. Key changes:
- Dashboard loads BSL config on mount (replaces hardcoded metadata)
- WebLLM generates structured queries validated against BSL schema
- Single-table queries execute locally via DuckDB WASM; multi-table joins route to the backend API
- New pipeline health dashboard surfaces observability data per tenant

### Phase 5: DuckDB WASM Local Enrichment
Enable users to upload CSV/Parquet files, load them into DuckDB WASM alongside star schema data fetched from the backend, and JOIN local files with platform data for ad-hoc enrichment. The semantic profiler auto-generates metadata for uploaded tables.

### Phase 6: dlt Lineage Integration
Link dlt load IDs through the observability models so each run result traces back to a specific data load. Surface end-to-end lineage (dlt load -> staging -> intermediate -> analytics) and use dlt schema metadata for automated drift detection.

**Phases 2 and 3 can run in parallel.** Phase 4 depends on both. Phases 5 and 6 are additive.

See [docs/roadmap.md](docs/roadmap.md) for the full implementation plan with file-level detail.

---

## Engine and Factory Contracts

### Paid Ads — Canonical Schema
All 7 paid ads engines normalize to:
```
tenant_slug | source_platform | report_date | campaign_id | ad_group_id | ad_id | spend | impressions | clicks | conversions
```

### Ecommerce — Canonical Schema
All 3 ecommerce engines normalize to:
```
tenant_slug | source_platform | order_id | order_date | total_price | currency | financial_status | customer_email | customer_id | line_items_json
```

### Analytics — Sessions Canonical Schema
All 3 analytics engines produce sessionized output:
```
tenant_slug | source_platform | session_id | user_pseudo_id | user_id | session_start_ts | session_end_ts | session_duration_seconds | events_in_session | traffic_source | traffic_medium | traffic_campaign | geo_country | device_category | is_conversion_session | session_revenue | transaction_id
```
Google Analytics and Mixpanel perform 30-minute inactivity sessionization. Amplitude uses native `session_id`. All accept a `conversion_events` list parameter for the `is_conversion_session` flag.

### Analytics — Events Canonical Schema
All 3 analytics engines also produce a raw event stream:
```
tenant_slug | source_platform | event_id | event_name | event_timestamp | user_pseudo_id | user_id | traffic_source | traffic_medium | traffic_campaign | geo_country | device_category | page_path | ecommerce_order_id | ecommerce_revenue | custom_properties
```

---

## Documentation

- [Roadmap](docs/roadmap.md) — Full implementation plan for frontend integration, query builder, observability, and enrichment features
